{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b68932-6cfb-45b7-8a44-c37007e1f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, lit, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a60e7be7-c909-43cb-a977-36f85e55638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session():\n",
    "    spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                        .appName(\"ny_bykeshares_analytics\") \\\n",
    "                        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb49527-5452-407f-b9f9-8c9011d70acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(df):\n",
    "    df = df.withColumn('date', split(df['starttime'], ' ').getItem(0))\n",
    "        \n",
    "    df = df.withColumn('year', split(df['date'], '/').getItem(2))\\\n",
    "        .withColumn('day', split(df['date'], '/').getItem(1))\\\n",
    "        .withColumn('month', split(df['date'], '/').getItem(0)).drop('date')\\\n",
    "        .withColumn('date_for_join', concat(col('day'), lit('-'), ('month'), lit('-'), col('year'))).drop(col('month')).drop(col('day'))\\\n",
    "        .drop('year')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa343f1-e6a4-4c54-86a9-ed816abf05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_df_columns(df_1, df_2):\n",
    "    flag = \"NOK!\"\n",
    "    if  df_1.count()  == df_2.count():\n",
    "        flag =\"OK!\"\n",
    "\n",
    "    if flag != \"OK!\":\n",
    "        raise Exception(\"Somethings is wrong with DFs\")\n",
    "    print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89008b31-dc40-45f0-9092-5a327853e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType,  FloatType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('tripduration', IntegerType()),\n",
    "    StructField('start_station_name', StringType()),\n",
    "    StructField('end_station_name', StringType()),\n",
    "    StructField('user_type', StringType()),\n",
    "    StructField('birthyear', IntegerType()),\n",
    "    StructField('gender', IntegerType()),\n",
    "    StructField('date', StringType()),\n",
    "    StructField('maximum_temperature', FloatType()),\n",
    "    StructField('minimum_temperature', FloatType()),\n",
    "    StructField('average_temperature', FloatType()),\n",
    "    StructField('precipitation', FloatType()),\n",
    "    StructField('snow_fall', FloatType()),\n",
    "    StructField('snow_depth', FloatType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6158bf98-dfe3-47e4-a970-f0b5ef92f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preanalytics_filter(df):\n",
    "    df = df.filter(df['tripduration'] > 0)\\\n",
    "    .filter(df['tripduration'] < 5000)\\\n",
    "    .filter(df['maximum_temperature'] > -25).filter(df['maximum_temperature'] < 125)\\\n",
    "    .filter(df['minimum_temperature'] > -25).filter(df['minimum_temperature'] < 125)\\\n",
    "    .filter(df['average_temperature'] > -25).filter(df['average_temperature'] < 125)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
